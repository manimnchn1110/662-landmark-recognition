{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from decode import read_and_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variable Setup\n",
    "train_whole_sample_size = 9996\n",
    "test_whole_sample_size = 9996\n",
    "gesture_class = len(df.landmark_id.unique())\n",
    "train_batch_size = 32\n",
    "test_batch_size = 32\n",
    "train_path = \"./Landmark_train.tfrecords\"\n",
    "graph_path = \"./tensorboard\"\n",
    "cnn_model_save_path = \"./cnn_model/cnn_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set\n",
    "img_train_batch, labels_train_batch = read_and_decode(train_path, train_batch_size, train_whole_sample_size)\n",
    "train_label = tf.one_hot(labels_train_batch, gesture_class, 1, 0)\n",
    "\n",
    "\n",
    "#test set\n",
    "img_test_batch, labels_test_batch = read_and_decode(train_path, test_batch_size, test_whole_sample_size)\n",
    "test_label = tf.one_hot(labels_test_batch, gesture_class, 1, 0)\n",
    "\n",
    "\n",
    "### create session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "### create graph placement\n",
    "x = tf.placeholder(tf.float32, shape=[None, 224, 224, 3], name=\"images\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, gesture_class,], name=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Layer Setup Function\n",
    "def weight_variable(shape, f_name):\n",
    "    initial = tf.truncated_normal(shape, mean=0, stddev=0.1)\n",
    "    return tf.Variable(initial, name=f_name)\n",
    "\n",
    "\n",
    "def bias_variable(shape, f_name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=f_name)\n",
    "\n",
    "\n",
    "def Conv2d_Filter(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def max_pooling_2x2(x, name):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_bias(name):\n",
    "    if data_dict is not None:\n",
    "        weight_name = \"_weight\"\n",
    "        bias_name = \"_bias\"\n",
    "        weight = tf.Variable(data_dict[name][0], name = name + weight_name)\n",
    "        bias =  tf.Variable(data_dict[name][1], name = name + bias_name)\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: Pre-trained VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = np.load(\"./vgg19.npy\", encoding='latin1').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1)conv1_1: conv 3x3 64 (RELU)\n",
    "- Input: 224x224x3\n",
    "- Type: Conv\n",
    "- size: 3x3\n",
    "- channel: 64\n",
    "- strides: 1\n",
    "- Output: 224x224x64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv1_1'):\n",
    "    #W_conv1 = weight_variable([3,3,3,64], 'W_conv1')\n",
    "    #b_conv1 = bias_variable([64], 'b_conv1')\n",
    "    W_conv1_1, b_conv1_1 = get_weight_bias('conv1_1')\n",
    "    with tf.name_scope('h_conv1'):\n",
    "        h_conv1_1 = tf.nn.relu(Conv2d_Filter(x, W_conv1_1) + b_conv1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2)conv1_2: conv 3x3 64 (RELU)\n",
    "\n",
    "- Input: 224x224x64\n",
    "- Type: Conv\n",
    "- size: 3x3\n",
    "- channel: 64\n",
    "- strides: 1\n",
    "- Output: 224x224x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv1_2'):\n",
    "    W_conv1_2, b_conv1_2 = get_weight_bias('conv1_2')\n",
    "    with tf.name_scope('h_conv2'):\n",
    "        h_conv1_2 = tf.nn.relu(Conv2d_Filter(h_conv1_1, W_conv1_2) + b_conv1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3)pool1: max pooling 2x2 2\n",
    "- Input: 224x224x64\n",
    "- pool: 2x2\n",
    "- output: 112x112x64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Pool1'):\n",
    "    h_pool1 = max_pooling_2x2(h_conv1_2, 'pool1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.1)conv2_1: conv 3x3 128 (RELU)\n",
    "- Input: 112x112x64\n",
    "- Type: Conv\n",
    "- size: 3x3\n",
    "- channel: 128\n",
    "- strides: 1\n",
    "- Output: 112x112x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv2_1'):\n",
    "    W_conv2_1, b_conv2_1 = get_weight_bias('conv2_1')\n",
    "    with tf.name_scope('h_conv2'):\n",
    "        h_conv2_1 = tf.nn.relu(Conv2d_Filter(h_pool1, W_conv2_1) + b_conv2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.1)conv2_2: conv 3x3 128 (RELU)\n",
    "- Input: 112x112x128\n",
    "- Type: Conv\n",
    "- size: 3x3\n",
    "- channel: 128\n",
    "- strides: 1\n",
    "- Output: 112x112x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv2_2'):\n",
    "    W_conv2_2, b_conv2_2 = get_weight_bias('conv2_2')\n",
    "    with tf.name_scope('h_conv2'):\n",
    "        h_conv2_2 = tf.nn.relu(Conv2d_Filter(h_conv2_1, W_conv2_2) + b_conv2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3)pool2: max pooling 2x2 2\n",
    "- Input: 112x112x128\n",
    "- pool: 2x2\n",
    "- output: 56x56x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Pool2'):\n",
    "    h_pool2 = max_pooling_2x2(h_conv2_2, 'pool2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1)conv3_1: conv 3x3 256 (RELU)\n",
    "- Input: 56x56x128\n",
    "- Type: Conv\n",
    "- size: 3x3\n",
    "- channel: 256\n",
    "- strides: 1\n",
    "- Output: 56x56x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv3_1'):\n",
    "    W_conv3_1, b_conv3_1 = get_weight_bias('conv3_1')\n",
    "    with tf.name_scope('h_conv3_1'):\n",
    "        h_conv3_1 = tf.nn.relu(Conv2d_Filter(h_pool2, W_conv3_1) + b_conv3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2)conv3_2: conv 3x3 256 (RELU)\n",
    "- Input: 56x56x256\n",
    "- Type: Conv\n",
    "- size: 3x3\n",
    "- channel: 256\n",
    "- strides: 1\n",
    "- Output: 56x56x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv3_2'):\n",
    "    W_conv3_2, b_conv3_2 = get_weight_bias('conv3_2')\n",
    "    with tf.name_scope('h_conv3_2'):\n",
    "        h_conv3_2 = tf.nn.relu(Conv2d_Filter(h_conv3_1, W_conv3_2) + b_conv3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3)conv3_3: conv 3x3 256 (RELU)\n",
    "- Input: 56x56x256\n",
    "- Type: Conv\n",
    "- size: 3x3\n",
    "- channel: 256\n",
    "- strides: 1\n",
    "- Output: 56x56x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv3_3'):\n",
    "    W_conv3_3, b_conv3_3 = get_weight_bias('conv3_3')\n",
    "    with tf.name_scope('h_conv3_3'):\n",
    "        h_conv3_3 = tf.nn.relu(Conv2d_Filter(h_conv3_2, W_conv3_3) + b_conv3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4)conv3_4: conv 3x3 256 (RELU)\n",
    "- Input: 56x56x256\n",
    "- Type: Conv\n",
    "- size: 3x3\n",
    "- channel: 256\n",
    "- strides: 1\n",
    "- Output: 56x56x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv3_4'):\n",
    "    W_conv3_4, b_conv3_4 = get_weight_bias('conv3_4')\n",
    "    with tf.name_scope('h_conv3_4'):\n",
    "        h_conv3_4 = tf.nn.relu(Conv2d_Filter(h_conv3_3, W_conv3_4) + b_conv3_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5)pool3: max pooling 2x2 2\n",
    "\"\"\"\n",
    "- Input: 56x56x256\n",
    "- pool: 2x2\n",
    "- output: 28x28x256\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Pool3'):\n",
    "    h_pool3 = max_pooling_2x2(h_conv3_4, 'pool3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1)conv4_1: conv 3x3 512 (RELU)\n",
    "- Input: 28x28x256\n",
    "- Type: Conv\n",
    "- size: 3x3\n",
    "- channel: 512\n",
    "- strides: 1\n",
    "- Output: 28x28x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv4_1'):\n",
    "    W_conv4_1, b_conv4_1 = get_weight_bias('conv4_1')\n",
    "    with tf.name_scope('h_conv4_1'):\n",
    "        h_conv4_1 = tf.nn.relu(Conv2d_Filter(h_pool3, W_conv4_1) + b_conv4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv4_2'):\n",
    "    W_conv4_2, b_conv4_2 = get_weight_bias('conv4_2')\n",
    "    with tf.name_scope('h_conv4_2'):\n",
    "        h_conv4_2 = tf.nn.relu(Conv2d_Filter(h_conv4_1, W_conv4_2) + b_conv4_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv4_3'):\n",
    "    W_conv4_3, b_conv4_3 = get_weight_bias('conv4_3')\n",
    "    with tf.name_scope('h_conv4_3'):\n",
    "        h_conv4_3 = tf.nn.relu(Conv2d_Filter(h_conv4_2, W_conv4_3) + b_conv4_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv4_4'):\n",
    "    W_conv4_4, b_conv4_4 = get_weight_bias('conv4_4')\n",
    "    with tf.name_scope('h_conv4_4'):\n",
    "        h_conv4_4 = tf.nn.relu(Conv2d_Filter(h_conv4_3, W_conv4_4) + b_conv4_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Pool4'):\n",
    "    h_pool4 = max_pooling_2x2(h_conv4_4, 'pool4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv5_1'):\n",
    "    W_conv5_1, b_conv5_1 = get_weight_bias('conv5_1')\n",
    "    with tf.name_scope('h_conv5_1'):\n",
    "        h_conv5_1 = tf.nn.relu(Conv2d_Filter(h_pool4, W_conv5_1) + b_conv5_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv5_2'):\n",
    "    W_conv5_2, b_conv5_2 = get_weight_bias('conv5_2')\n",
    "    with tf.name_scope('h_conv5_2'):\n",
    "        h_conv5_2 = tf.nn.relu(Conv2d_Filter(h_conv5_1, W_conv5_2) + b_conv5_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv5_3'):\n",
    "    W_conv5_3, b_conv5_3 = get_weight_bias('conv5_3')\n",
    "    with tf.name_scope('h_conv5_3'):\n",
    "        h_conv5_3 = tf.nn.relu(Conv2d_Filter(h_conv5_2, W_conv5_3) + b_conv5_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('conv5_4'):\n",
    "    W_conv5_4, b_conv5_4 = get_weight_bias('conv5_4')\n",
    "    with tf.name_scope('h_conv5_4'):\n",
    "        h_conv5_4 = tf.nn.relu(Conv2d_Filter(h_conv5_3, W_conv5_4) + b_conv5_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Pool5'):\n",
    "    h_pool5 = max_pooling_2x2(h_conv5_4, 'pool5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"fc6\"):\n",
    "    W_fc6, b_fc6 = get_weight_bias('fc6')\n",
    "    \n",
    "    with tf.name_scope('Pool5_flat'):\n",
    "        h_pool6_flat = tf.reshape(h_pool5, [-1, 25088])\n",
    "\n",
    "    with tf.name_scope('h_fc1'):\n",
    "        h_fc6 = tf.nn.relu(tf.matmul(h_pool6_flat, W_fc6) + b_fc6)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    h_fc6_drop = tf.nn.dropout(h_fc6, keep_prob, name=\"h_fc6_drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"fc7\"):\n",
    "    W_fc7, b_fc7 = get_weight_bias('fc7')\n",
    "    \n",
    "    with tf.name_scope('Pool5_flat'):\n",
    "        h_pool7_flat = tf.reshape(h_fc6_drop, [-1, 4096])\n",
    "\n",
    "    with tf.name_scope('h_fc1'):\n",
    "        h_fc7 = tf.nn.relu(tf.matmul(h_pool7_flat, W_fc7) + b_fc7)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    h_fc7_drop = tf.nn.dropout(h_fc7, keep_prob, name=\"h_fc6_drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  13)Layer 8: Readout Layer (Softmax Layer)\n",
    "with tf.name_scope('fc8'):\n",
    "    W_fc8 = weight_variable([4096, gesture_class], 'fc8')\n",
    "    b_fc8 = bias_variable([gesture_class], 'fc8')\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(tf.matmul(h_fc7_drop, W_fc8) + b_fc8, name=\"my_prediction\")\n",
    "\n",
    "# Define functions and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Corss_Entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction),\n",
    "                                   name=\"loss\")\n",
    "    tf.summary.scalar('corss_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('Train_step'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy, name=\"train_step\")\n",
    "\n",
    "correct_prediction = tf.equal(tf.arg_max(prediction, 1), tf.arg_max(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "###Start to Train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(graph_path, sess.graph);\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    max_acc = 0\n",
    "\n",
    "    for i in range(1001):\n",
    "\n",
    "        img_xs, label_xs = sess.run([img_train_batch, train_label])\n",
    "        sess.run(train_step, feed_dict={x: img_xs, y: label_xs, keep_prob: 0.75})\n",
    "\n",
    "        if (i % 10) == 0:\n",
    "            print(\"The\", i, \"Train\")\n",
    "            img_test_xs, label_test_xs = sess.run([img_test_batch, test_label])\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_test_xs, y: label_test_xs, keep_prob: 1.0})\n",
    "            print(\"Itsers = \" + str(i) + \"  Accuracy: \" + str(acc))\n",
    "\n",
    "            summay = sess.run(merged, feed_dict={x: img_test_xs, y: label_test_xs, keep_prob: 1.0})\n",
    "\n",
    "            train_writer.add_summary(summay, i)\n",
    "\n",
    "            if max_acc < acc:\n",
    "                max_acc = acc\n",
    "                saver.save(sess, save_path=cnn_model_save_path)\n",
    "\n",
    "            if acc > 0.50:\n",
    "                break\n",
    "\n",
    "    train_writer.close()\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    saver.save(sess, save_path=cnn_model_save_path)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
